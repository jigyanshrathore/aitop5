# Importing necessary libraries
import pandas as pd
import numpy as np
import json

# Importing modules for text processing and similarity calculations
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Importing the 're' library for text cleaning
import re

# Loading the CSV file containing questions and answers into a DataFrame
questions_df = pd.read_csv('clearfeed_qa_pairs.csv')

# Displaying the first few rows of the DataFrame to verify data loading
questions_df.head()

# Loading the JSON file containing document data
with open('Clearfeed_kb.json', 'r') as f:
    doc_data = json.load(f)

# Function to clean and preprocess text by converting to lowercase and removing special characters
def clean_text(input_text):
    input_text = input_text.lower()  # Convert text to lowercase
    input_text = re.sub(r'[^\w\s]', '', input_text)  # Remove special characters
    return input_text

# Applying the clean_text function to the 'question' column in the DataFrame
questions_df['processed_question'] = questions_df['question'].apply(clean_text)

# Extracting and cleaning text content from the loaded document data
doc_content = {url: clean_text(data['title'] + " " + data['text']) for url, data in doc_data.items()}

# Separating the URLs and corresponding cleaned text for further processing
doc_urls = list(doc_content.keys())
doc_texts = list(doc_content.values())

# Initializing a TF-IDF vectorizer to convert text data into numerical representations
text_vectorizer = TfidfVectorizer(max_features=2000, stop_words='english')

# Creating a TF-IDF matrix for the document texts
doc_tfidf_matrix = text_vectorizer.fit_transform(doc_texts)

# Function to find the top N most similar documents to a given query
def top_5_matches(query, vectorizer, tfidf_matrix, url_list, num_results=5):
    # Vectorizing the input query text
    query_vector = vectorizer.transform([clean_text(query)])
    
    # Calculating cosine similarity between the query and document vectors
    similarity_scores = cosine_similarity(query_vector, tfidf_matrix).flatten()
    
    # Finding the indices of the top N similar documents
    top_indices = similarity_scores.argsort()[-num_results:][::-1]
    
    # Retrieving the URLs of the top N documents
    top_urls = [url_list[i] for i in top_indices]
    return top_urls

# Example query to test the similarity search (IndexError may occur if index exceeds the DataFrame's range)
example_query = questions_df['question'].iloc[55]  # Replace 55 with a valid index if this raises an error

# Retrieving the top URLs for the example query
top_urls_example = top_5_matches(example_query, text_vectorizer, doc_tfidf_matrix, doc_urls)

# Printing the example query and its top matching document URLs
print("Example Query:", example_query)
print("Top Matching URLs:", top_urls_example)
